![](D:\保存位置\markdwon笔记图片保存内容\CNN-2.png)

#### 输入形式

$$
[B,C,H,W]
$$



1. 深入理解卷积

> 从INPUT->C1，我们可以发现C1有**6个channel**即**（6个卷积核组，同一组只有一个bias）（一个卷积核组指的是对于INPUT中不同通道数构成的过滤器的组合，最终需要将一个卷积组得到的结果相加，再加bias）**。即C1的weight的shape为[6, 1, 5, 5]，bias.shape=[6]。而对于S2->C3的卷积过程有**16个channel即（16个卷积组，每个卷积组有6个不同的过滤期争对S2的6个通道，最终不同过滤器得到的结果相加，并且加上卷积组的bias），所以C3.weight .shape=[16, 6, 5, 5], C3.bias.shape=[16]。**
>
> ```python
> 
> import torch
> #创建input
> a = torch.rand(1, 3, 32, 32)*10
> #创建卷积层，1个卷积组（3个过滤器）
> layler = torch.nn.Conv2d(3, 1, 5, stride=1, padding=0)
> #weight.shape [1, 3, 5, 5]
> w = layler.weight
> #[1]
> b = layler.bias
> 
> #卷积之后的输出 [1, 1, 28, 28]
> layler(a).shape
> 
> #堆叠1个卷积组中3个不同过滤期的结果
> bb = 0
> for i in range(3):
>  bb += torch.dot(w[:,i,...].flatten(), a[:,i,:5,:5].flatten()).item()
> 
> #将过滤器得到的结果+通道偏执  与  卷积层输出的结果进行比对
> print((bb+b[0]).item() - layler(a).flatten()[0].item())
> > -9.5367431640625e-07 #表明我们的想法是准确的
> ```
>
> ![](D:\保存位置\markdwon笔记图片保存内容\CNN.jpg)
>
> ### 线性层的weight与bias
>
> ```python
> # layer.weight.shape = [out, in], bias.shape = [out]
> layer = nn.Linear(2, 4)
> #则layer.weight.shape=(4, 2), layer.bias.shape=[4]
> 
> #网络参数的储存
> a = nn.Sequential(
> 	nn.Linear(4, 2),
>     nn.Linear(2, 6)
> )
> #具体一点是索引值为0，1分别储存了第一个线性层的weight，与bias
> #2, 3分别储存了第二个线性层的weight，bias
> list(a.parameters()).shape = [4]
> 
> #以元组形式返回每一个调优参数的名字以及参数tensor
>  list(a.named_parameters())
> ```
>
> 

2. 卷积特征提取过程

![](D:\保存位置\markdwon笔记图片保存内容\cnn_feature.png)

1. ### 卷积过后的图像大小

$$
h_{out}={{h_{in}+2*padding-h\_kernel + 1} \over strider} \\
w_{out}={{w_{in}+2*padding-w\_kernel+1} \over strider}
$$

### 网络性能比较

![](D:\保存位置\markdwon笔记图片保存内容\capacity.png)

1. 1*1的kernel主要用来做更改通道数的作用
2. 3\*3,5\*5的kernel-size事实证明也有较好的效果，big size 也不一定好，而且计算耗时

### Resnet(残差网络/短路网络

通过构造段落，来确保堆叠神经元层数的同时也能至少保证不比以前层数低效果差的结果

![](D:\保存位置\markdwon笔记图片保存内容\resnet.png)

```python
#实现上图的三个卷积层的短路设计

class Res(nn.Module):
    def __init__(self, cha_in, cha_out):
        super().__init__()
        self.conv1 = nn.Conv2d(cha_in, cha_out, kernel_size=1, stride=1)
        self.conv2 = nn.Conv2d(cha_out, cha_out, kernel_size=3, stride=1, padding=1)
        self.conv3 = nn.Conv2d(cha_out, 256, kernel_size=1, stride=1)
        self.bn1 = nn.BatchNorm2d(cha_out)
        self.bn1 = nn.BatchNorm2d(cha_out)
        self.bn3 = nn.BatchNorm2d(256)
        self.extra = nn.Sequential()
        if cha_in != 256: #如果输入通道！=256,进行1*1的kernel来转换通道数
            self.extra = nn.Sequential(
                nn.Conv2d(cha_in, 256, kernel_size=1, stride=1),
                nn.BatchNorm2d(256)
                )
            
    
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = F.relu(self.bn3(self.conv3(out)))
        sout = self.extra(x) + out
        return out
```

